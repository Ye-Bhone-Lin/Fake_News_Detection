# -*- coding: utf-8 -*-
"""Fake news detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1231PqIXg4P0r4xSO7Hba2PgnHOMczNyR

###Import the library and data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pip install pandas



pip install afinn

from google.colab import drive
drive.mount('/content/drive')

fake_news = pd.read_csv('/content/drive/My Drive/Simbolo Project Bootcamp/Fake news detection/Fake.csv')
true_news = pd.read_csv('/content/drive/My Drive/Simbolo Project Bootcamp/Fake news detection/True.csv')

"""### Data Preparation"""

fake_news['classification'] = 0
true_news['classification'] = 1

fake_news.drop_duplicates(inplace=True)

true_news.drop_duplicates(inplace=True)

fake_news = fake_news.drop(labels=['title','date'],axis=1)
true_news = true_news.drop(labels=['title','date'],axis=1)



fake_news.head()

true_news.head()



fake_news.info()

fake_news.subject.unique()

true_news.subject.unique()

fake_news.isnull().sum()
true_news.isnull().sum()

fake_news.subject.value_counts()

true_news.subject.value_counts()

df = pd.concat([true_news,fake_news],axis=0)
df

df.info()

df = df.sample(frac = 1)
df.reset_index(inplace=True)
df.drop(['index'],axis=1,inplace=True)

fake_classifi = df.loc[df.classification == 0]
fake_classifi = fake_classifi.classification.count()
fake_classifi

true_classifi = df.loc[df.classification == 1]
true_classifi = true_classifi.classification.count()
true_classifi

"""### Exploratory Data Analysis"""

plt.figure(figsize=(10,5))
sns.countplot(fake_news['subject'],data = fake_news)
plt.tight_layout()
plt.title('Visualization of fake news')
ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), 
            fontsize=12, ha='center', va='bottom')
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(true_news['subject'],data = true_news)
plt.tight_layout()
plt.title('Visualization of true news')
ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), 
            fontsize=12, ha='center', va='bottom')
plt.show()

plt.figure(figsize=(11,6))
sns.countplot(df['classification'],data = df)
plt.tight_layout()
plt.title('Count by True or Fake news')
ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), 
            fontsize=12, ha='center', va='bottom')
plt.show()

plt.figure(figsize=(14,6))
sns.countplot(df['classification'],data = df,hue='subject')
plt.tight_layout()
plt.title('True or Fake News with Barplot Visualization')
plt.show()

"""###Statistical analysis """

c = df.loc[df.classification == 0]
c1 = df.loc[df.classification == 1]
print('Total Fake news:' ,c.classification.count())
print('Total True news:' , c1.classification.count())

pd.crosstab(df.classification,df.subject)

"""### Prediction by using Random Forest Classifier"""

import re
import string
def clean_text(text):
  text = re.sub('\[.*?\]', '', text)
  text = re.sub("\\W"," ",text) 
  text = re.sub('https?://\S+|www\.\S+', '', text)
  text = re.sub('<.*?>+', '', text)
  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
  text = re.sub('\n', '', text)
  text = re.sub('\w*\d\w*', '', text) 
  text = re.sub('[^A-Za-z]+',' ',text)
  text = text.lower()
  return text
df['clean_text'] = df['text'].apply(clean_text) # df['text'] = df['text'].apply(clean_text)

df.head()

x = df['clean_text']
y = df['classification']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y,stratify=y, test_size=0.25,random_state=0)

x_test

x_train

x_train.shape

y_train.shape

y_test

from sklearn.feature_extraction.text import TfidfVectorizer
vectorization = TfidfVectorizer()
x_train_cv = vectorization.fit_transform(x_train)
x_test_cv = vectorization.transform(x_test)

x_train_cv.shape

y_train.shape

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=0)
clf.fit(x_train_cv, y_train)

predict_y = clf.predict(x_test_cv)

clf.score(x_test_cv, y_test)

test_msg = 'North Korea has launched 23 missiles in 16 different tests in the months since Donald Trump took office. The most recent test, in late November, was of a ballistic missile that could reach the U.S. mainland. Kim Jong Un appears to be making major strides in his nuclear program, and the uptick in test launches prompted Trump to dub him "Little Rocket Man."'
text = clean_text(test_msg)

text = str(text)
print(clf.predict(vectorization.transform([text])))

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
def score(actual, pred):
    Accuracy_Score = accuracy_score(actual, pred)  
    Precision_Score = precision_score(actual, pred)
    Recall_Score = recall_score(actual, pred)
    F1_score = f1_score(actual, pred)
    
    print("Accuracy : " + "%.4f" % Accuracy_Score)
    print( "Precision : " "%.4f" % Precision_Score)
    print( "Recall : " "%.4f" % Recall_Score)
    print( "F1 : " "%.4f" % F1_score)

score(predict_y, y_test)

"""### Prediction by using LogisticRegression"""

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression()
LR.fit(x_train_cv, y_train)

pred_LR = LR.predict(x_test_cv)

score(pred_LR, y_test)

"""### Prediction by using DecisionTreeClassifier"""

from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier()
DT.fit(x_train_cv, y_train)

pred_DT = DT.predict(x_test_cv)

score(pred_DT, y_test)

"""### Sentiment Analysis by using Afinn"""

#from afinn import Afinn
#afn = Afinn()
#text = ['very happy','happy','extremely happy','sad']
#scores = [afn.score(text) for text in text]
#sentiment = ['positive' if score > 0 else 'negative' if score < 0 else 'neutral' for score in scores]
#print(sentiment)
#print(scores)

#x_emotion = df['clean_text']
#scores = [afn.score(text) for text in x_emotion]
#sentiment = ['positive' if score > 0 else 'negative' if score < 0 else 'neutral' for score in scores]
#print(sentiment)
#print(scores)

#df['Sentiment'] = sentiment
#df['Scores'] = scores

#df.head()

#print(df['Scores'].max())
#print(df['Scores'].min())

"""### Multiclass classification"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

STOPWORDS = set(stopwords.words('english'))
def remove_stopword(text):
  text = ' '.join(word for word in text.split() if word not in STOPWORDS)
  return text

df['stopwords'] = df['clean_text'].apply(remove_stopword)

df.head()

pip install tensorflow

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

MAX_NB_WORDS = 50000
MAX_SEQUENCE_LENGTH = 400
EMBEDDING_DIM = 100
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True,char_level=False,filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~')
tokenizer.fit_on_texts(df['stopwords'])

X = tokenizer.texts_to_sequences(df['stopwords'].values)

X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print(X.shape)
X

Y = pd.get_dummies(df['subject']).values
print(Y.shape)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 0)

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SpatialDropout1D, Dense, LSTM,Bidirectional, Dropout

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(8, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

num_epochs = 5
early_stop = EarlyStopping(monitor='val_loss', patience=3,min_delta=0.0001)
batch_size = 50
model.fit(X_train, Y_train, epochs=num_epochs, batch_size=batch_size,validation_split=0.1,callbacks=early_stop)

text = ["In March, then-FBI Director James Comey confirmed the agency was investigating Russian efforts to influence the 2016 election and possible ties between the Trump campaign and the Kremlin. In May, the Justice Department appointed former FBI Director Robert Mueller as a special counsel to oversee the probe days after Trump fired Comey. At every step, Trump has called the investigation a witch hunt designed to discredit his electoral victory. The probe has led to charges filed against former Trump campaign manager Paul Manafort and ex-national security advisor Michael Flynn, though the charges are unrelated to collusion with Moscow during the campaign."]
seq = tokenizer.texts_to_sequences(text)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
pred = model.predict(padded)
labels = ['News', 'politics', 'Government News', 'left-news', 'US_News',
       'Middle-east','wordlnews','politicsNews']
print(pred, labels[np.argmax(pred)])

accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

model.save('/content/drive/My Drive/Simbolo Project Bootcamp/Fake news detection/multi_model')

#ls '/content/drive/My Drive/Simbolo Project Bootcamp/Fake news detection'

#new_model = tf.keras.models.load_model('/content/drive/My Drive/Simbolo Project Bootcamp/Fake news detection/multi_model')

